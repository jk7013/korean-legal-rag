# 오답 분석 리포트

실험 설정: KCL-MCQA 283문제, bge-m3 임베딩, top_k=5

---

## 1. o4-mini / bge-m3 — "판례는 찾았는데 틀린 문제" 분석

### 배경

Recall@5=True(상위 5개 안에 정답 판례 포함)인데 correct=False인 경우는
"검색은 성공했지만 LLM 추론이 실패한" 케이스다. 이 갭은 리랭킹이나 프롬프트 개선으로
좁힐 수 있는 **잠재 성능**이기도 하다.

### 집계

| 항목 | 수치 |
|------|------|
| Recall@5=True인 문제 | 144개 |
| 그 중 정답 | 80개 (55.6%) |
| **그 중 오답 (분석 대상)** | **64개 (44.4%)** |
| 전체 정확도 | 45.9% |

> o4-mini가 정답 판례를 찾고도 틀린 비율: **44.4%** — 검색이 완벽해도 추론 한계가 존재

### 과목별 분포

| 과목 | 오답 수 | Recall@5=True 전체 대비 |
|------|---------|------------------------|
| 민사법 | 34개 | - |
| 형사법 | 18개 | - |
| 공법 | 12개 | - |
| **합계** | **64개** | - |

### 오답 거리 분포 (|예측 - 정답|)

| 거리 | 개수 | 비율 |
|------|------|------|
| 1 (인접 선택지) | 31개 | 48.4% |
| 2 | 15개 | 23.4% |
| 3 | 10개 | 15.6% |
| 4 (반대쪽 끝) | 8개 | 12.5% |

→ 절반 가까이(48%)가 **인접한 선택지(거리 1)로 틀림**. 완전히 엉뚱한 선택이 아니라 "비슷하게 그럴듯한 선택지"에 속은 케이스. 정답 판례를 5개 중 어느 위치에서 읽느냐(리랭킹)에 따라 결과가 달라질 수 있음.

### 정답 번호 vs 예측 번호 분포

| 번호 | 정답(실제) | 예측(o4-mini) |
|------|-----------|--------------|
| 1 | 12개 | 15개 |
| 2 | 11개 | 14개 |
| 3 | 13개 | 12개 |
| 4 | 12개 | 16개 |
| 5 | **16개** | **7개** |

→ **5번 정답인 문제가 16개(25%)로 가장 많은데, 5번으로 예측한 것은 7개뿐(11%)**.
o4-mini가 5번 선택지를 체계적으로 기피하는 경향이 있음. 정답 판례가 있어도 마지막 선택지를 소극적으로 선택하는 편향(position bias) 의심.

### 문제 목록 (question_id / 과목 / 예측 / 정답)

#### 공법 (12개)

| question_id | 예측 | 정답 |
|-------------|------|------|
| q_106 | 4 | 2 |
| q_107 | 3 | 1 |
| q_115 | 2 | 4 |
| q_128 | 1 | 3 |
| q_132 | 4 | 2 |
| q_245 | 5 | 4 |
| q_259 | 4 | 5 |
| q_263 | 1 | 2 |
| q_264 | 5 | 2 |
| q_272 | 2 | 1 |
| q_275 | 4 | 1 |
| q_282 | 5 | 2 |

#### 민사법 (34개)

| question_id | 예측 | 정답 |
|-------------|------|------|
| q_002 | 4 | 3 |
| q_012 | 4 | 5 |
| q_014 | 4 | 3 |
| q_015 | 1 | 5 |
| q_016 | 1 | 4 |
| q_020 | 1 | 5 |
| q_025 | 1 | 5 |
| q_028 | 3 | 5 |
| q_029 | 1 | 5 |
| q_037 | 2 | 1 |
| q_045 | 4 | 3 |
| q_046 | 4 | 3 |
| q_054 | 3 | 1 |
| q_055 | 3 | 5 |
| q_057 | 4 | 5 |
| q_061 | 3 | 2 |
| q_144 | 1 | 2 |
| q_150 | 4 | 2 |
| q_152 | 3 | 4 |
| q_155 | 1 | 2 |
| q_158 | 3 | 4 |
| q_160 | 3 | 5 |
| q_166 | 4 | 1 |
| q_167 | 4 | 3 |
| q_169 | 1 | 2 |
| q_171 | 2 | 1 |
| q_185 | 1 | 4 |
| q_188 | 2 | 3 |
| q_190 | 2 | 5 |
| q_192 | 3 | 2 |
| q_193 | 2 | 3 |
| q_196 | 4 | 3 |
| q_197 | 1 | 5 |
| q_199 | 2 | 4 |

#### 형사법 (18개)

| question_id | 예측 | 정답 |
|-------------|------|------|
| q_075 | 2 | 4 |
| q_078 | 2 | 3 |
| q_079 | 3 | 4 |
| q_080 | 1 | 5 |
| q_084 | 5 | 3 |
| q_088 | 2 | 1 |
| q_090 | 5 | 1 |
| q_091 | 2 | 5 |
| q_094 | 5 | 4 |
| q_099 | 2 | 3 |
| q_209 | 1 | 5 |
| q_215 | 1 | 3 |
| q_216 | 3 | 5 |
| q_217 | 2 | 1 |
| q_218 | 4 | 1 |
| q_219 | 3 | 4 |
| q_225 | 4 | 1 |
| q_239 | 5 | 4 |

---

## 2. Hybrid vs Retrieved/bge-m3 (GPT-4o-mini) 전환 분석

### 배경

Hybrid(벡터 + BM25 + RRF)는 Recall@5를 50.9%→67.1%(+16.2%p) 크게 올렸지만
Accuracy는 32.2%→32.9%(+0.7%p) 소폭 향상에 그쳤다. 실제로 문제별로 어떤 전환이 발생했는지 분석.

### 전환 집계

| 항목 | 개수 |
|------|------|
| 전체 비교 문제 | 283개 |
| 둘 다 정답 | 69개 (24.4%) |
| 둘 다 오답 | 168개 (59.4%) |
| **Hybrid 새로 맞은 문제** | **24개** (Retrieved 틀림 → Hybrid 맞음) |
| **Hybrid 새로 틀린 문제** | **22개** (Retrieved 맞음 → Hybrid 틀림) |
| **순증가** | **+2문제** (+24 − 22) |

→ Accuracy +0.7%p(+2문제)는 24개를 새로 맞고 22개를 새로 틀린 교환의 결과.
교란 규모가 크다 — 전체의 약 16%(46/283)가 Hybrid 추가로 답이 뒤바뀜.

### Hybrid 새로 틀린 22개의 recall 분석

| recall_at_5 | 개수 | 해석 |
|-------------|------|------|
| True (판례 찾았는데 틀림) | 17개 | BM25 추가 판례가 컨텍스트 오염 |
| False (판례도 못 찾고 틀림) | 5개 | 검색 자체도 실패 |

→ 새로 틀린 22개 중 **17개(77%)는 정답 판례를 찾아놓고도 틀렸다**.
BM25가 끌어온 키워드 매칭 판례들이 의미적으로 무관할 경우, 정답 판례가 5개 중 묻혀서
LLM이 판단을 잘못 내리는 **컨텍스트 희석** 현상으로 해석된다.

### Hybrid 새로 맞은 24개 — 과목별 및 예측 전환

| question_id | 과목 | 정답 | Retrieved 예측 | Hybrid 예측 |
|-------------|------|------|---------------|------------|
| q_255 | 공법 | 1 | 3 | 1 |
| q_258 | 공법 | 2 | 1 | 2 |
| q_277 | 공법 | 1 | 3 | 1 |
| q_001 | 민사법 | 3 | 1 | 3 |
| q_006 | 민사법 | 3 | 1 | 3 |
| q_016 | 민사법 | 4 | 2 | 4 |
| q_018 | 민사법 | 2 | 5 | 2 |
| q_024 | 민사법 | 3 | 1 | 3 |
| q_031 | 민사법 | 5 | 2 | 5 |
| q_036 | 민사법 | 2 | 4 | 2 |
| q_050 | 민사법 | 3 | 2 | 3 |
| q_056 | 민사법 | 2 | 1 | 2 |
| q_057 | 민사법 | 5 | 4 | 5 |
| q_063 | 민사법 | 4 | 3 | 4 |
| q_143 | 민사법 | 5 | 3 | 5 |
| q_145 | 민사법 | 5 | 2 | 5 |
| q_198 | 민사법 | 2 | 1 | 2 |
| q_202 | 민사법 | 3 | 2 | 3 |
| q_067 | 형사법 | 4 | 2 | 4 |
| q_084 | 형사법 | 3 | 1 | 3 |
| q_102 | 형사법 | 1 | 2 | 1 |
| q_104 | 형사법 | 2 | 3 | 2 |
| q_227 | 형사법 | 3 | 4 | 3 |
| q_233 | 형사법 | 5 | 3 | 5 |

과목별: 민사법 15개 / 형사법 6개 / 공법 3개

### Hybrid 새로 틀린 22개 — 과목별 및 예측 전환

| question_id | 과목 | 정답 | Retrieved 예측 | Hybrid 예측 |
|-------------|------|------|---------------|------------|
| q_118 | 공법 | 1 | 1✓ | 2 |
| q_128 | 공법 | 3 | 3✓ | 4 |
| q_249 | 공법 | 5 | 5✓ | 1 |
| q_261 | 공법 | 3 | 3✓ | 2 |
| q_000 | 민사법 | 4 | 4✓ | 1 |
| q_003 | 민사법 | 3 | 3✓ | 4 |
| q_022 | 민사법 | 4 | 4✓ | 3 |
| q_037 | 민사법 | 1 | 1✓ | 2 |
| q_062 | 민사법 | 5 | 5✓ | 3 |
| q_064 | 민사법 | 1 | 1✓ | 2 |
| q_142 | 민사법 | 3 | 3✓ | 2 |
| q_151 | 민사법 | 1 | 1✓ | 3 |
| q_155 | 민사법 | 2 | 2✓ | 4 |
| q_185 | 민사법 | 4 | 4✓ | 1 |
| q_192 | 민사법 | 2 | 2✓ | 3 |
| q_073 | 형사법 | 1 | 1✓ | 4 |
| q_074 | 형사법 | 2 | 2✓ | 1 |
| q_077 | 형사법 | 5 | 5✓ | 1 |
| q_093 | 형사법 | 5 | 5✓ | 1 |
| q_217 | 형사법 | 1 | 1✓ | 2 |
| q_223 | 형사법 | 2 | 2✓ | 3 |
| q_230 | 형사법 | 1 | 1✓ | 2 |

과목별: 민사법 11개 / 형사법 7개 / 공법 4개

특이점: 새로 틀린 22개 중 **예측이 1번으로 바뀐 경우 7개** — BM25가 끌어온 판례 중
공통적으로 1번 선택지를 지지하는 방향의 판례가 많이 섞인 것으로 추정.

---

## 3. 종합 인사이트

### 인사이트 A: Reasoning 모델의 추론 실패 원인은 "판례 내 신호 경쟁"

o4-mini가 Recall@5=True인데 틀린 64개에서 오답 거리 1이 48%를 차지한다.
이는 "완전히 엉뚱한 선택"이 아니라 "비슷하게 그럴듯한 선택지 사이의 판단 실패"다.
5개 판례 중 정답과 관련된 판례가 1개, 나머지 4개가 비슷한 사안의 다른 판결을 담고 있을 때,
Reasoning 모델이 깊이 추론할수록 오히려 노이즈 신호에 수렴하는 위험이 있다.

→ **해결 방향**: 상위 1~2개로 컨텍스트를 좁히는 리랭킹, 또는 CoT에서 "어느 판례를 근거로 썼는지" 명시하게 하는 프롬프트 개선.

### 인사이트 B: 5번 선택지 과소예측 — Position Bias

정답이 5번인 문제 16개 중 5번으로 예측한 것은 7개(44%). 다른 번호(1~4번)는 정답=예측 비율이
더 균형적이다. Reasoning 모델도 마지막 선택지를 소극적으로 고르는 편향이 존재할 수 있으며,
이는 Few-shot 예시에 5번 정답을 포함하면 어느 정도 완화 가능하다.

### 인사이트 C: Hybrid의 교란 효과 — 46개(16%)가 답 전환

Hybrid가 새로 맞은 24개와 새로 틀린 22개는 BM25가 추가한 판례가 각각 "올바른 신호"와
"잘못된 노이즈"로 작용한 케이스다. 새로 틀린 22개 중 77%(17개)는 recall_at_5=True임에도
틀렸다 — 정답 판례가 컨텍스트에 있는데도 BM25 판례들이 주의를 분산시켰다.

→ **해결 방향**: RRF로 합산된 5개를 그대로 쓰는 대신, Cross-Encoder 리랭킹으로
정답 판례를 1번 위치로 올리는 것이 Hybrid Accuracy 한계를 돌파하는 핵심.

### 인사이트 D: Recall→Accuracy 전환율의 현실

| 설정 | Recall@5 | Accuracy | 전환율(Recall@5 중 정답) |
|------|----------|----------|--------------------------|
| o4-mini / bge-m3 | 50.9% (144/283) | 45.9% | 55.6% (80/144) |
| o4-mini / Hybrid | 67.1% (190/283) | 44.9% | — |

Recall@5=50.9%에서 전환율 55.6%로 Accuracy 45.9%가 나온다.
Hybrid로 Recall@5=67.1%까지 올렸지만 Accuracy는 오히려 하락 → 전환율이 더 낮아진 것.
Hybrid가 추가로 찾은 46개 판례(190-144)의 전환율이 매우 낮음을 의미한다.
이 46개에서 Accuracy를 끌어내려면 리랭킹으로 신호 대 노이즈 비율을 높여야 한다.

---

## 4. 다음 실험 제안

| 우선순위 | 실험 | 기대 효과 |
|----------|------|-----------|
| ★★★ | Cross-Encoder 리랭킹 (top-5 → top-1 재순위) | Recall@5=67.1%를 Accuracy로 전환 |
| ★★ | top_k=1로 줄이기 (노이즈 최소화) | 컨텍스트 희석 제거, Precision 극대화 |
| ★★ | 프롬프트 개선 (5번 선택지 균형 + 판례 출처 명시) | Position bias + 신호 경쟁 완화 |
| ★ | HyDE + Hybrid 조합 | Recall@5 추가 개선 가능성 확인 |
