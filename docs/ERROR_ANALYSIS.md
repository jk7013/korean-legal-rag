# 오답 분석 리포트

실험 설정: KCL-MCQA 283문제, bge-m3 임베딩, top_k=5

---

## 1. o4-mini / bge-m3 — "판례는 찾았는데 틀린 문제" 분석

### 배경

Recall@5=True(상위 5개 안에 정답 판례 포함)인데 correct=False인 경우는
"검색은 성공했지만 LLM 추론이 실패한" 케이스다. 이 갭은 리랭킹이나 프롬프트 개선으로
좁힐 수 있는 **잠재 성능**이기도 하다.

### 집계

| 항목 | 수치 |
|------|------|
| Recall@5=True인 문제 | 144개 |
| 그 중 정답 | 80개 (55.6%) |
| **그 중 오답 (분석 대상)** | **64개 (44.4%)** |
| 전체 정확도 | 45.9% |

> o4-mini가 정답 판례를 찾고도 틀린 비율: **44.4%** — 검색이 완벽해도 추론 한계가 존재

### 과목별 분포

| 과목 | 오답 수 | Recall@5=True 전체 대비 |
|------|---------|------------------------|
| 민사법 | 34개 | - |
| 형사법 | 18개 | - |
| 공법 | 12개 | - |
| **합계** | **64개** | - |

### 오답 거리 분포 (|예측 - 정답|)

| 거리 | 개수 | 비율 |
|------|------|------|
| 1 (인접 선택지) | 31개 | 48.4% |
| 2 | 15개 | 23.4% |
| 3 | 10개 | 15.6% |
| 4 (반대쪽 끝) | 8개 | 12.5% |

→ 절반 가까이(48%)가 **인접한 선택지(거리 1)로 틀림**. 완전히 엉뚱한 선택이 아니라 "비슷하게 그럴듯한 선택지"에 속은 케이스. 정답 판례를 5개 중 어느 위치에서 읽느냐(리랭킹)에 따라 결과가 달라질 수 있음.

### 정답 번호 vs 예측 번호 분포

| 번호 | 정답(실제) | 예측(o4-mini) |
|------|-----------|--------------|
| 1 | 12개 | 15개 |
| 2 | 11개 | 14개 |
| 3 | 13개 | 12개 |
| 4 | 12개 | 16개 |
| 5 | **16개** | **7개** |

→ **5번 정답인 문제가 16개(25%)로 가장 많은데, 5번으로 예측한 것은 7개뿐(11%)**.
o4-mini가 5번 선택지를 체계적으로 기피하는 경향이 있음. 정답 판례가 있어도 마지막 선택지를 소극적으로 선택하는 편향(position bias) 의심.

### 문제 목록 (question_id / 과목 / 예측 / 정답)

#### 공법 (12개)

| question_id | 예측 | 정답 |
|-------------|------|------|
| q_106 | 4 | 2 |
| q_107 | 3 | 1 |
| q_115 | 2 | 4 |
| q_128 | 1 | 3 |
| q_132 | 4 | 2 |
| q_245 | 5 | 4 |
| q_259 | 4 | 5 |
| q_263 | 1 | 2 |
| q_264 | 5 | 2 |
| q_272 | 2 | 1 |
| q_275 | 4 | 1 |
| q_282 | 5 | 2 |

#### 민사법 (34개)

| question_id | 예측 | 정답 |
|-------------|------|------|
| q_002 | 4 | 3 |
| q_012 | 4 | 5 |
| q_014 | 4 | 3 |
| q_015 | 1 | 5 |
| q_016 | 1 | 4 |
| q_020 | 1 | 5 |
| q_025 | 1 | 5 |
| q_028 | 3 | 5 |
| q_029 | 1 | 5 |
| q_037 | 2 | 1 |
| q_045 | 4 | 3 |
| q_046 | 4 | 3 |
| q_054 | 3 | 1 |
| q_055 | 3 | 5 |
| q_057 | 4 | 5 |
| q_061 | 3 | 2 |
| q_144 | 1 | 2 |
| q_150 | 4 | 2 |
| q_152 | 3 | 4 |
| q_155 | 1 | 2 |
| q_158 | 3 | 4 |
| q_160 | 3 | 5 |
| q_166 | 4 | 1 |
| q_167 | 4 | 3 |
| q_169 | 1 | 2 |
| q_171 | 2 | 1 |
| q_185 | 1 | 4 |
| q_188 | 2 | 3 |
| q_190 | 2 | 5 |
| q_192 | 3 | 2 |
| q_193 | 2 | 3 |
| q_196 | 4 | 3 |
| q_197 | 1 | 5 |
| q_199 | 2 | 4 |

#### 형사법 (18개)

| question_id | 예측 | 정답 |
|-------------|------|------|
| q_075 | 2 | 4 |
| q_078 | 2 | 3 |
| q_079 | 3 | 4 |
| q_080 | 1 | 5 |
| q_084 | 5 | 3 |
| q_088 | 2 | 1 |
| q_090 | 5 | 1 |
| q_091 | 2 | 5 |
| q_094 | 5 | 4 |
| q_099 | 2 | 3 |
| q_209 | 1 | 5 |
| q_215 | 1 | 3 |
| q_216 | 3 | 5 |
| q_217 | 2 | 1 |
| q_218 | 4 | 1 |
| q_219 | 3 | 4 |
| q_225 | 4 | 1 |
| q_239 | 5 | 4 |

---

## 2. LLM-as-a-Judge 실패 유형 분류 (o4-mini + bge-m3)

### 목적

섹션 1의 64개 케이스("판례는 찾았는데 틀린 문제")를 gpt-4o-mini Judge가
질적으로 분류. 수치 분석(오답 거리, position bias)에서 알 수 없었던
**"왜 틀렸는가"**를 4가지 유형으로 구분.

### 분류 기준

| 유형 | 정의 |
|------|------|
| Misinterpretation | 정답 판례를 읽었지만 법적 해석 자체를 잘못함 |
| Distraction | 유사한 선택지 때문에 정답 판례를 올바르게 적용하지 못함 |
| Noise Dominance | 다른 판례(오답 판례)를 정답 판례보다 더 신뢰함 |
| Irrelevant | 판례 내용이 이 문제 해결에 실질적으로 도움이 되지 않음 |

### 집계 결과

| Error Type        | Count | Ratio  |
|-------------------|-------|--------|
| Misinterpretation | 35개   | 54.7%  |
| Distraction       | 29개   | 45.3%  |
| Noise Dominance   |  0개   |  0.0%  |
| Irrelevant        |  0개   |  0.0%  |
| **합계**           | **64개** | **100%** |

### 해석

**Noise Dominance = 0개**

Recall@5=True 케이스이므로 정답 판례가 컨텍스트에 있었는데도 "다른 판례에 끌려간
경우가 없다"고 Judge가 판단했다. 즉 판례 우선순위 문제가 아니라, 판례를 제대로
읽고도 결론을 잘못 내리는 추론 실패가 본질이다.

**Misinterpretation (54.7%) — 법적 추론 자체의 한계**

정답 판례의 법리는 읽었지만 5지선다 맥락에서 어떤 선택지가 그 법리에 부합하는지
판단에 실패한 케이스. 한국 변호사시험 특유의 정밀한 법적 해석 요구 수준이 원인.
임베딩 모델이나 검색 파이프라인 개선으로는 해결 불가 — 모델의 법률 도메인 능력 문제.

**Distraction (45.3%) — 선택지 트랩**

정답 판례를 올바르게 해석했더라도, 유사 선택지가 만든 혼동에 의해 잘못된 번호를
고른 케이스. 섹션 1의 "오답 거리 1이 48%" 패턴과 일치한다.
Few-shot에 유사 선택지 구별 예시를 포함하거나, CoT에서 "어느 선택지를 왜 소거했는지"
명시하게 하는 프롬프트 개선으로 부분 완화 가능.

### 인사이트: 리랭킹의 한계

이 결과는 **리랭킹이 Recall→Accuracy 전환을 개선하더라도 한계가 있음**을 시사한다.
정답 판례를 top-1으로 올려도 Misinterpretation(54.7%)은 해결되지 않는다.
리랭킹은 Noise Dominance가 주된 원인일 때 효과적인데, 이 실험에서는 Noise Dominance가 0%.
실질적인 Accuracy 개선을 위해서는 법률 도메인 특화 파인튜닝 또는 더 강한 추론 모델이 필요하다.

---

## 3. Hybrid vs Retrieved/bge-m3 (GPT-4o-mini) 전환 분석

### 배경

Hybrid(벡터 + BM25 + RRF)는 Recall@5를 50.9%→67.1%(+16.2%p) 크게 올렸지만
Accuracy는 32.2%→32.9%(+0.7%p) 소폭 향상에 그쳤다. 실제로 문제별로 어떤 전환이 발생했는지 분석.

### 전환 집계

| 항목 | 개수 |
|------|------|
| 전체 비교 문제 | 283개 |
| 둘 다 정답 | 69개 (24.4%) |
| 둘 다 오답 | 168개 (59.4%) |
| **Hybrid 새로 맞은 문제** | **24개** (Retrieved 틀림 → Hybrid 맞음) |
| **Hybrid 새로 틀린 문제** | **22개** (Retrieved 맞음 → Hybrid 틀림) |
| **순증가** | **+2문제** (+24 − 22) |

→ Accuracy +0.7%p(+2문제)는 24개를 새로 맞고 22개를 새로 틀린 교환의 결과.
교란 규모가 크다 — 전체의 약 16%(46/283)가 Hybrid 추가로 답이 뒤바뀜.

### Hybrid 새로 틀린 22개의 recall 분석

| recall_at_5 | 개수 | 해석 |
|-------------|------|------|
| True (판례 찾았는데 틀림) | 17개 | BM25 추가 판례가 컨텍스트 오염 |
| False (판례도 못 찾고 틀림) | 5개 | 검색 자체도 실패 |

→ 새로 틀린 22개 중 **17개(77%)는 정답 판례를 찾아놓고도 틀렸다**.
BM25가 끌어온 키워드 매칭 판례들이 의미적으로 무관할 경우, 정답 판례가 5개 중 묻혀서
LLM이 판단을 잘못 내리는 **컨텍스트 희석** 현상으로 해석된다.

### Hybrid 새로 맞은 24개 — 과목별 및 예측 전환

| question_id | 과목 | 정답 | Retrieved 예측 | Hybrid 예측 |
|-------------|------|------|---------------|------------|
| q_255 | 공법 | 1 | 3 | 1 |
| q_258 | 공법 | 2 | 1 | 2 |
| q_277 | 공법 | 1 | 3 | 1 |
| q_001 | 민사법 | 3 | 1 | 3 |
| q_006 | 민사법 | 3 | 1 | 3 |
| q_016 | 민사법 | 4 | 2 | 4 |
| q_018 | 민사법 | 2 | 5 | 2 |
| q_024 | 민사법 | 3 | 1 | 3 |
| q_031 | 민사법 | 5 | 2 | 5 |
| q_036 | 민사법 | 2 | 4 | 2 |
| q_050 | 민사법 | 3 | 2 | 3 |
| q_056 | 민사법 | 2 | 1 | 2 |
| q_057 | 민사법 | 5 | 4 | 5 |
| q_063 | 민사법 | 4 | 3 | 4 |
| q_143 | 민사법 | 5 | 3 | 5 |
| q_145 | 민사법 | 5 | 2 | 5 |
| q_198 | 민사법 | 2 | 1 | 2 |
| q_202 | 민사법 | 3 | 2 | 3 |
| q_067 | 형사법 | 4 | 2 | 4 |
| q_084 | 형사법 | 3 | 1 | 3 |
| q_102 | 형사법 | 1 | 2 | 1 |
| q_104 | 형사법 | 2 | 3 | 2 |
| q_227 | 형사법 | 3 | 4 | 3 |
| q_233 | 형사법 | 5 | 3 | 5 |

과목별: 민사법 15개 / 형사법 6개 / 공법 3개

### Hybrid 새로 틀린 22개 — 과목별 및 예측 전환

| question_id | 과목 | 정답 | Retrieved 예측 | Hybrid 예측 |
|-------------|------|------|---------------|------------|
| q_118 | 공법 | 1 | 1✓ | 2 |
| q_128 | 공법 | 3 | 3✓ | 4 |
| q_249 | 공법 | 5 | 5✓ | 1 |
| q_261 | 공법 | 3 | 3✓ | 2 |
| q_000 | 민사법 | 4 | 4✓ | 1 |
| q_003 | 민사법 | 3 | 3✓ | 4 |
| q_022 | 민사법 | 4 | 4✓ | 3 |
| q_037 | 민사법 | 1 | 1✓ | 2 |
| q_062 | 민사법 | 5 | 5✓ | 3 |
| q_064 | 민사법 | 1 | 1✓ | 2 |
| q_142 | 민사법 | 3 | 3✓ | 2 |
| q_151 | 민사법 | 1 | 1✓ | 3 |
| q_155 | 민사법 | 2 | 2✓ | 4 |
| q_185 | 민사법 | 4 | 4✓ | 1 |
| q_192 | 민사법 | 2 | 2✓ | 3 |
| q_073 | 형사법 | 1 | 1✓ | 4 |
| q_074 | 형사법 | 2 | 2✓ | 1 |
| q_077 | 형사법 | 5 | 5✓ | 1 |
| q_093 | 형사법 | 5 | 5✓ | 1 |
| q_217 | 형사법 | 1 | 1✓ | 2 |
| q_223 | 형사법 | 2 | 2✓ | 3 |
| q_230 | 형사법 | 1 | 1✓ | 2 |

과목별: 민사법 11개 / 형사법 7개 / 공법 4개

특이점: 새로 틀린 22개 중 **예측이 1번으로 바뀐 경우 7개** — BM25가 끌어온 판례 중
공통적으로 1번 선택지를 지지하는 방향의 판례가 많이 섞인 것으로 추정.

---

## 4. 종합 인사이트

### 인사이트 A: Reasoning 모델의 추론 실패 원인은 "판례 내 신호 경쟁"

o4-mini가 Recall@5=True인데 틀린 64개에서 오답 거리 1이 48%를 차지한다.
이는 "완전히 엉뚱한 선택"이 아니라 "비슷하게 그럴듯한 선택지 사이의 판단 실패"다.
5개 판례 중 정답과 관련된 판례가 1개, 나머지 4개가 비슷한 사안의 다른 판결을 담고 있을 때,
Reasoning 모델이 깊이 추론할수록 오히려 노이즈 신호에 수렴하는 위험이 있다.

→ **해결 방향**: 상위 1~2개로 컨텍스트를 좁히는 리랭킹, 또는 CoT에서 "어느 판례를 근거로 썼는지" 명시하게 하는 프롬프트 개선.

### 인사이트 B: 5번 선택지 과소예측 — Position Bias

정답이 5번인 문제 16개 중 5번으로 예측한 것은 7개(44%). 다른 번호(1~4번)는 정답=예측 비율이
더 균형적이다. Reasoning 모델도 마지막 선택지를 소극적으로 고르는 편향이 존재할 수 있으며,
이는 Few-shot 예시에 5번 정답을 포함하면 어느 정도 완화 가능하다.

### 인사이트 C: Hybrid의 교란 효과 — 46개(16%)가 답 전환

Hybrid가 새로 맞은 24개와 새로 틀린 22개는 BM25가 추가한 판례가 각각 "올바른 신호"와
"잘못된 노이즈"로 작용한 케이스다. 새로 틀린 22개 중 77%(17개)는 recall_at_5=True임에도
틀렸다 — 정답 판례가 컨텍스트에 있는데도 BM25 판례들이 주의를 분산시켰다.

→ **해결 방향**: RRF로 합산된 5개를 그대로 쓰는 대신, Cross-Encoder 리랭킹으로
정답 판례를 1번 위치로 올리는 것이 Hybrid Accuracy 한계를 돌파하는 핵심.

### 인사이트 D: Recall→Accuracy 전환율의 현실

| 설정 | Recall@5 | Accuracy | 전환율(Recall@5 중 정답) |
|------|----------|----------|--------------------------|
| o4-mini / bge-m3 | 50.9% (144/283) | 45.9% | 55.6% (80/144) |
| o4-mini / Hybrid | 67.1% (190/283) | 44.9% | — |

Recall@5=50.9%에서 전환율 55.6%로 Accuracy 45.9%가 나온다.
Hybrid로 Recall@5=67.1%까지 올렸지만 Accuracy는 오히려 하락 → 전환율이 더 낮아진 것.
Hybrid가 추가로 찾은 46개 판례(190-144)의 전환율이 매우 낮음을 의미한다.
이 46개에서 Accuracy를 끌어내려면 리랭킹으로 신호 대 노이즈 비율을 높여야 한다.

---

## 5. Essay LLM-as-a-Judge 결과 (Vanilla / Oracle / Retrieved)

### 목적

KCL-Essay 169문제에 대해 3가지 세팅(Vanilla / Oracle / Retrieved/bge-m3)으로
에세이를 생성하고, gpt-4o-mini Judge가 3가지 기준으로 1~5점 채점.
MCQA에서 확인한 Oracle↔Retrieved RAG 갭이 서술형 에세이에서도 나타나는지 검증.

### 평가 기준

| 기준 | 설명 |
|------|------|
| Legal Accuracy | 법적 내용의 정확성 — 관련 법리와 판례를 올바르게 적용했는가 |
| Reasoning Quality | 추론의 체계성 — 쟁점 → 법리 → 결론의 논리 구조가 명확한가 |
| Citation Fidelity | 판례 인용 충실성 — 제공된 판례를 근거로 사용했는가 (환각 없는가) |

### 집계 결과 (n=169문제, 각 1~5점)

| Setting           |   N | Legal Acc | Reasoning | Citation |  Avg  |
|-------------------|-----|-----------|-----------|----------|-------|
| Vanilla           | 169 |      3.09 |      3.19 |     2.18 |  2.82 |
| Oracle            | 169 |      3.98 |      3.79 |     3.83 |  3.87 |
| Retrieved/bge-m3  | 169 |      3.24 |      3.30 |     2.78 |  3.11 |

### 해석

**Oracle vs Vanilla 갭: +1.05점 (2.82→3.87)**

판례를 직접 주입했을 때 평균 1점 이상 향상. 특히 Citation Fidelity가
2.18→3.83(+1.65)으로 가장 크게 오름 — 판례가 없으면 LLM이 판례를 지어내거나
언급 자체를 피하는 반면, Oracle 세팅에서는 실제 판례를 구체적으로 인용한다.

**Retrieved vs Vanilla 갭: +0.29점 (2.82→3.11)**

검색으로 찾은 판례도 도움이 되지만 Oracle 대비 +0.76점(3.11 vs 3.87) 부족.
이는 MCQA에서 확인한 Oracle↔Retrieved 갭 패턴과 일치한다.

**Citation Fidelity의 Oracle↔Retrieved 격차: 3.83 vs 2.78 (+1.05)**

MCQA용 bge-m3 인덱스는 짧게 추출된 판례 스니펫을 저장하고 있어,
Oracle(HuggingFace 전문 판례)에 비해 정보 밀도가 낮다.
Retrieved 세팅에서 Citation 점수가 낮은 것은 검색 자체의 한계이기도 하다.

**Legal Accuracy / Reasoning Quality: 세팅 간 격차 상대적으로 작음**

Legal Accuracy(3.09→3.24→3.98), Reasoning Quality(3.19→3.30→3.79)는
Citation 대비 세팅 간 차이가 작다. 500자 에세이 과제에서 법적 추론 구조 자체는
판례 유무와 무관하게 어느 정도 유지되는 것으로 해석된다.

---

## 6. 다음 실험 제안

| 우선순위 | 실험 | 기대 효과 |
|----------|------|-----------|
| ★★★ | Cross-Encoder 리랭킹 (top-5 → top-1 재순위) | Recall@5=67.1%를 Accuracy로 전환 |
| ★★ | top_k=1로 줄이기 (노이즈 최소화) | 컨텍스트 희석 제거, Precision 극대화 |
| ★★ | 프롬프트 개선 (5번 선택지 균형 + 판례 출처 명시) | Position bias + 신호 경쟁 완화 |
| ★ | HyDE + Hybrid 조합 | Recall@5 추가 개선 가능성 확인 |
